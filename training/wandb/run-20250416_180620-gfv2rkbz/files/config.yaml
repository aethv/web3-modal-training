_wandb:
    value:
        cli_version: 0.19.9
        m: []
        python_version: 3.12.8
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 55
                - 71
                - 98
            "2":
                - 1
                - 11
                - 49
                - 51
                - 55
                - 71
                - 98
            "3":
                - 16
                - 23
                - 55
            "4": 3.12.8
            "5": 0.19.9
            "6": 4.48.0
            "8":
                - 5
            "12": 0.19.9
            "13": darwin-arm64
base_model_name:
    value: codellama/CodeLlama-7b-hf
batch_size:
    value: 2
data_dir:
    value: ./data
data_source_path:
    value: ./raw_data
eos_token:
    value: </s>
eval_batch_size:
    value: 2
eval_steps:
    value: 500
existing_dataset_path:
    value: ""
feedback_dir:
    value: ./feedback
gradient_accumulation_steps:
    value: 8
learning_rate:
    value: 2e-05
logging_steps:
    value: 100
lora_config:
    value:
        bias: none
        lora_alpha: 32
        lora_dropout: 0.05
        r: 16
        task_type: CAUSAL_LM
max_length:
    value: 2048
model_dir:
    value: ./models
num_epochs:
    value: 3
output_dir:
    value: ./outputs
prompt_prefix:
    value: '### Instruction: '
prompt_suffix:
    value: ' ### Response: '
save_steps:
    value: 500
save_total_limit:
    value: 3
test_split_ratio:
    value: 0.1
training_examples_required:
    value: 1000
use_existing_dataset:
    value: false
use_lora:
    value: true
weight_decay:
    value: 0.01
